{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "german_to_english_translate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramapriyakp/Portfolio/blob/master/NLP/Neural%20machine%20translator%20English-German%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdiqXVo-w9o2",
        "colab_type": "text"
      },
      "source": [
        "# Language Translation with Word Level seq2seq DL Models\n",
        "\n",
        "The objective is to convert a German sentence (sequence of words) to English using a Neural Machine Translation (NMT) system based on word level encoder-decoder models.\n",
        "\n",
        "![](https://i.imgur.com/So32H7U.png)\n",
        "\n",
        "We will use German-English sentence pairs data from http://www.manythings.org/anki/\n",
        "\n",
        "\n",
        "Sequence-to-Sequence (seq2seq) models are used for a variety of NLP tasks, such as text summarization, speech recognition, language translation, text-to-speech, speech-to-text among others. Our aim is to translate german to english sentences.\n",
        "\n",
        "Here, both, the input and output are sentences. In other words, these sentences are a sequence of words going in and out of our model.\n",
        "\n",
        "![](https://i.imgur.com/Uk1tCPo.png)\n",
        "\n",
        "A typical seq2seq model is also known as an encoder-decoder model and has 2 major components:\n",
        "- The encoder \n",
        "- The decoder\n",
        "\n",
        "Both these parts are essentially two different sequential models like RNNs\\LSTMs which are combined together.\n",
        "\n",
        "![](https://i.imgur.com/bT6PAtv.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcMK6bJZyIWj",
        "colab_type": "code",
        "outputId": "f8b792bb-cdfe-4c49-80dc-e59ac3f38979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEZydXjxx2Kk",
        "colab_type": "code",
        "outputId": "e17f4649-7f1d-47ed-f3bb-8ab3c35af073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/NLP"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NLP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjvGuKHWBP5b",
        "colab_type": "text"
      },
      "source": [
        "Download the data and extract \"deu.txt\" in your working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkbaVx0fzFFL",
        "colab_type": "code",
        "outputId": "e572283f-ed1e-49ae-9fbc-4f69249754ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!wget http://www.manythings.org/anki/deu-eng.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-09 07:30:59--  http://www.manythings.org/anki/deu-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 104.24.109.196, 2606:4700:30::6818:6cc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4541707 (4.3M) [application/zip]\n",
            "Saving to: ‘deu-eng.zip.1’\n",
            "\n",
            "\rdeu-eng.zip.1         0%[                    ]       0  --.-KB/s               \rdeu-eng.zip.1        23%[===>                ]   1.01M  5.08MB/s               \rdeu-eng.zip.1       100%[===================>]   4.33M  15.1MB/s    in 0.3s    \n",
            "\n",
            "2019-09-09 07:31:00 (15.1 MB/s) - ‘deu-eng.zip.1’ saved [4541707/4541707]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOzR3_pdzJdP",
        "colab_type": "code",
        "outputId": "ce89c8b3-5820-4776-9e2d-4d0f6794a39b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip deu-eng.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  deu-eng.zip\n",
            "replace deu.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3REBlmShA7po",
        "colab_type": "text"
      },
      "source": [
        "##Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0s913AiyrrM",
        "colab_type": "code",
        "outputId": "c1752d10-4411-4ed5-a804-779f20186cb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import string\n",
        "import re\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASP4ouTHyzyz",
        "colab_type": "text"
      },
      "source": [
        "# Load up the dataset\n",
        "\n",
        "Our data is a text file of English-German sentence pairs. First we will read the file using the function defined below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbke9p8fy56W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to read raw text file\n",
        "def read_text(filename):\n",
        "    # open the file\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnN1TkQ3zV8V",
        "colab_type": "text"
      },
      "source": [
        "Now let's define a function to split the text into English-German pairs separated by '\\n' and then split these pairs into English sentences and German sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r_Cuct6zagq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split a text into sentences\n",
        "def to_lines(text):\n",
        "    sents = text.strip().split('\\n')\n",
        "    sents = [i.split('\\t') for i in sents]\n",
        "    return sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWm7Jpw3zi0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = read_text(\"deu.txt\")\n",
        "deu_eng = to_lines(data)\n",
        "deu_eng = array(deu_eng)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03mBoij8znIo",
        "colab_type": "text"
      },
      "source": [
        "The actual data contains over 150,000 sentence-pairs. However, we will use the first 50,000 sentence pairs only to reduce the training time of the model. You can change this number as per you system computation power."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4UrE8LIzpUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deu_eng = deu_eng[:50000,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmzKAtcXzxK-",
        "colab_type": "text"
      },
      "source": [
        "# Text Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7f9Gra8z12S",
        "colab_type": "text"
      },
      "source": [
        "**Text Cleaning**\n",
        "\n",
        "Let's take a look at our data, then we will decide which pre-processing steps to adopt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ_9Fm5iz8Ib",
        "colab_type": "code",
        "outputId": "2b701e60-5555-4054-8a7f-896547c43d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "deu_eng"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['Hi.', 'Hallo!'],\n",
              "       ['Hi.', 'Grüß Gott!'],\n",
              "       ['Run!', 'Lauf!'],\n",
              "       ...,\n",
              "       ['The man died of cancer.', 'Der Mann starb an Krebs.'],\n",
              "       ['The man lay motionless.', 'Der Mann lag bewegungslos da.'],\n",
              "       ['The man must be insane.', 'Der Mann muss geistesgestört sein.']],\n",
              "      dtype='<U537')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDOD_csO0DNg",
        "colab_type": "text"
      },
      "source": [
        "We will get rid of the punctuation marks, and then convert the text to lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZQS4iHk0HiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove punctuation\n",
        "deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]]\n",
        "deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdy8VDxk0LPo",
        "colab_type": "code",
        "outputId": "d38112ab-8c6e-4100-d2fc-0af6d6474900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "deu_eng"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['Hi', 'Hallo'],\n",
              "       ['Hi', 'Grüß Gott'],\n",
              "       ['Run', 'Lauf'],\n",
              "       ...,\n",
              "       ['The man died of cancer', 'Der Mann starb an Krebs'],\n",
              "       ['The man lay motionless', 'Der Mann lag bewegungslos da'],\n",
              "       ['The man must be insane', 'Der Mann muss geistesgestört sein']],\n",
              "      dtype='<U537')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mfqf1Tk0Q9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert to lowercase\n",
        "for i in range(len(deu_eng)):\n",
        "    deu_eng[i,0] = deu_eng[i,0].lower()\n",
        "    \n",
        "    deu_eng[i,1] = deu_eng[i,1].lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EOv-Upq0URl",
        "colab_type": "code",
        "outputId": "b1e2f9c4-a095-4089-9c15-5e2d406f76b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "deu_eng"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['hi', 'hallo'],\n",
              "       ['hi', 'grüß gott'],\n",
              "       ['run', 'lauf'],\n",
              "       ...,\n",
              "       ['the man died of cancer', 'der mann starb an krebs'],\n",
              "       ['the man lay motionless', 'der mann lag bewegungslos da'],\n",
              "       ['the man must be insane', 'der mann muss geistesgestört sein']],\n",
              "      dtype='<U537')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2CQpoIR0aLT",
        "colab_type": "text"
      },
      "source": [
        "##Text to Sequence Conversion\n",
        "\n",
        "To feed our data in a Seq2Seq model, we will have to convert both the input and the output sentences into integer sequences of fixed length. Before that, let's visualise the length of the sentences. We will capture the lengths of all the sentences in two separate lists for English and German, respectively.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFwVvzZg0rJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# empty lists\n",
        "eng_l = []\n",
        "deu_l = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in deu_eng[:,0]:\n",
        "    eng_l.append(len(i.split()))\n",
        "\n",
        "for i in deu_eng[:,1]:\n",
        "    deu_l.append(len(i.split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sIwCE_h0ySL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length_df = pd.DataFrame({'eng':eng_l, 'deu':deu_l})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3beEzQ4P02rR",
        "colab_type": "code",
        "outputId": "d147faea-81dd-4f2b-adfe-a0cdb0432973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nODmYmVnB/wefhgP5omFI5gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBFU5Dq607-2",
        "colab_type": "text"
      },
      "source": [
        "The maximum length of the German sentences is 11 and that of the English phrases is 8.\n",
        "\n",
        "Let's vectorize our text data by using Keras's Tokenizer() class. It will turn our sentences into sequences of integers. Then we will pad those sequences with zeros to make all the sequences of same length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKNGr1jG1BMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rU0BxBU1FWV",
        "colab_type": "code",
        "outputId": "ede3c343-8cd7-4391-e1ee-0b13992caeb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(deu_eng[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "eng_length = 8\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 6352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC8wRdTl1LOi",
        "colab_type": "code",
        "outputId": "5bb4a4e1-d493-41d4-ac95-075c6e1a5b85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# prepare Deutch tokenizer\n",
        "deu_tokenizer = tokenization(deu_eng[:, 1])\n",
        "deu_vocab_size = len(deu_tokenizer.word_index) + 1\n",
        "\n",
        "deu_length = 8\n",
        "print('Deutch Vocabulary Size: %d' % deu_vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deutch Vocabulary Size: 10678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiwImp8x1PA9",
        "colab_type": "text"
      },
      "source": [
        "Given below is a function to prepare the sequences. It will also perform sequence padding to a maximum sentence length as mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utZzSCli1TDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    # integer encode sequences\n",
        "    seq = tokenizer.texts_to_sequences(lines)\n",
        "    # pad sequences with 0 values\n",
        "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "    return seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ5mT8fM1ZVI",
        "colab_type": "text"
      },
      "source": [
        "##Model Building\n",
        "\n",
        "We will now split the data into train and test set for model training and evaluation, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8nYBu4z1d5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(deu_eng, test_size=0.2, random_state = 12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yP8W1Gz1l02",
        "colab_type": "text"
      },
      "source": [
        "It's time to encode the sentences. We will encode German sentences as the input sequences and English sentences as the target sequences. It will be done for both train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKCdVAuk1nbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare training data\n",
        "trainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hptKVNuv1ubj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare validation data\n",
        "testX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSXTr7ryKEW",
        "colab_type": "text"
      },
      "source": [
        "# Build seq2seq Model Architecture\n",
        "​\n",
        "![](https://i.imgur.com/3ZVi97s.png)\n",
        "​\n",
        "- For the encoder, we will use an embedding layer and an LSTM layer\n",
        "- For the decoder, we will use another LSTM layer followed by a dense layer\n",
        "- Repeat Vector helps pass the output sequence from encoder to all LSTM cells in the decoder\n",
        "​\n",
        "We leverage the full power of GPUs by using the CUDA variant of the LSTM models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZgnw9N91yJ6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Now comes the exciting part! Let us define our Seq2Seq model architecture. We are using an Embedding layer and an LSTM layer as our encoder and another LSTM layer followed by a Dense layer as the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRLdksqbDen8",
        "colab_type": "text"
      },
      "source": [
        "##Seq2seq:\n",
        "Seq2seq was introduced for machine translation, by Google. Before that, translation worked in a very naïve way. Each word typed was converted to its target language giving no regard to its grammar and sentence structure. Seq2seq revolutionized the process of translation by making use of deep learning. It not only takes the current word/input into account while translating but also its neighborhood.\n",
        "\n",
        "Nowadays, it is used for a variety of different applications such as image captioning, conversational models, text summarization etc.\n",
        "\n",
        "##Seq2seq Working:\n",
        "As the name suggests, seq2seq takes as input a sequence of words(sentence or sentences) and generates an output sequence of words. It does so by use of the recurrent neural network (RNN). Although the vanilla version of RNN is rarely used, its more advanced version i.e. LSTM or GRU are used. This is because RNN suffers from the problem of vanishing gradient. LSTM is used in the version proposed by Google. It develops the context of the word by taking 2 inputs at each point of time. One from the user and other from its previous output, hence the name recurrent (output goes as input).\n",
        "\n",
        "It mainly has two components i.e encoder and decoder, and hence sometimes it is called the Encoder-Decoder Network.\n",
        "\n",
        "*  __Encoder__: It uses deep neural network layers and converts the input words to corresponding hidden vectors. Each vector represents the current word  \n",
        "* __Decoder__: It is similar to the encoder. It takes as input the hidden vector generated by encoder, its own hidden states and current word to produce the next hidden vector and finally predict the next word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuRUOV3uEp4a",
        "colab_type": "text"
      },
      "source": [
        "Apart from these two, many optimizations have lead to other components of seq2seq:\n",
        "\n",
        "**Attention:** The input to the decoder is a single vector which has to store all the information about the context. This becomes a problem with large sequences. Hence the attention mechanism is applied which allows the decoder to look at the input sequence selectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImlcYuaI10Dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build NMT model\n",
        "def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
        "    model.add(LSTM(units))\n",
        "    model.add(RepeatVector(out_timesteps))\n",
        "    model.add(LSTM(units, return_sequences=True))\n",
        "    model.add(Dense(out_vocab, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0crCEcL1677",
        "colab_type": "text"
      },
      "source": [
        "We are using RMSprop optimizer in this model as it is usually a good choice for recurrent neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OfGMUVc18pQ",
        "colab_type": "code",
        "outputId": "acc18fb8-ffc1-405e-bf6e-39e9756a0357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "model = build_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)\n",
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umcLmBTZ2Dm2",
        "colab_type": "text"
      },
      "source": [
        "Please note that we have used **'sparse_categorical_crossentropy'** as the loss function because it allows us to use the target sequence as it is instead of one hot encoded format. One hot encoding the target sequences with such a huge vocabulary might consume our system's entire memory.\n",
        "\n",
        "Now we are all set to start training our model. We will train it for 30 epochs and with a batch size of 512. You may change and play these hyperparameters. We  also  use **ModelCheckpoint()** to save the best model with lowest validation loss. I personally prefer this method over early stopping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K14c71YZyfeu",
        "colab_type": "text"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "We save the model with minimum loss here also with checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fye0jDck2P1c",
        "colab_type": "code",
        "outputId": "eb3e9ac2-5b18-4336-cba8-d9bb7be2e853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "filename = 'model.ge_en'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), \n",
        "          epochs=30, batch_size=512, \n",
        "          validation_split = 0.2,\n",
        "          callbacks=[checkpoint], verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Train on 32000 samples, validate on 8000 samples\n",
            "Epoch 1/30\n",
            "32000/32000 [==============================] - 21s 663us/step - loss: 3.5739 - val_loss: 3.0018\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.00182, saving model to model.ge_en\n",
            "Epoch 2/30\n",
            "32000/32000 [==============================] - 15s 475us/step - loss: 2.9078 - val_loss: 2.8995\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.00182 to 2.89952, saving model to model.ge_en\n",
            "Epoch 3/30\n",
            "32000/32000 [==============================] - 15s 474us/step - loss: 2.7349 - val_loss: 2.6755\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.89952 to 2.67547, saving model to model.ge_en\n",
            "Epoch 4/30\n",
            "32000/32000 [==============================] - 15s 475us/step - loss: 2.5431 - val_loss: 2.5531\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.67547 to 2.55309, saving model to model.ge_en\n",
            "Epoch 5/30\n",
            "32000/32000 [==============================] - 15s 473us/step - loss: 2.3790 - val_loss: 2.4252\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.55309 to 2.42524, saving model to model.ge_en\n",
            "Epoch 6/30\n",
            "32000/32000 [==============================] - 15s 474us/step - loss: 2.2357 - val_loss: 2.3029\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.42524 to 2.30291, saving model to model.ge_en\n",
            "Epoch 7/30\n",
            "32000/32000 [==============================] - 15s 474us/step - loss: 2.1077 - val_loss: 2.2109\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.30291 to 2.21088, saving model to model.ge_en\n",
            "Epoch 8/30\n",
            "32000/32000 [==============================] - 15s 473us/step - loss: 1.9869 - val_loss: 2.1276\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.21088 to 2.12760, saving model to model.ge_en\n",
            "Epoch 9/30\n",
            "32000/32000 [==============================] - 15s 472us/step - loss: 1.8744 - val_loss: 2.0714\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.12760 to 2.07139, saving model to model.ge_en\n",
            "Epoch 10/30\n",
            "32000/32000 [==============================] - 15s 472us/step - loss: 1.7674 - val_loss: 1.9917\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.07139 to 1.99171, saving model to model.ge_en\n",
            "Epoch 11/30\n",
            "32000/32000 [==============================] - 15s 473us/step - loss: 1.6651 - val_loss: 1.9414\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.99171 to 1.94140, saving model to model.ge_en\n",
            "Epoch 12/30\n",
            "32000/32000 [==============================] - 15s 474us/step - loss: 1.5648 - val_loss: 1.8674\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.94140 to 1.86743, saving model to model.ge_en\n",
            "Epoch 13/30\n",
            "32000/32000 [==============================] - 15s 472us/step - loss: 1.4708 - val_loss: 1.8095\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.86743 to 1.80950, saving model to model.ge_en\n",
            "Epoch 14/30\n",
            "32000/32000 [==============================] - 15s 471us/step - loss: 1.3797 - val_loss: 1.7492\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.80950 to 1.74921, saving model to model.ge_en\n",
            "Epoch 15/30\n",
            "32000/32000 [==============================] - 15s 473us/step - loss: 1.2926 - val_loss: 1.7301\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.74921 to 1.73012, saving model to model.ge_en\n",
            "Epoch 16/30\n",
            "32000/32000 [==============================] - 15s 473us/step - loss: 1.2102 - val_loss: 1.6635\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.73012 to 1.66351, saving model to model.ge_en\n",
            "Epoch 17/30\n",
            "32000/32000 [==============================] - 15s 473us/step - loss: 1.1313 - val_loss: 1.6262\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.66351 to 1.62618, saving model to model.ge_en\n",
            "Epoch 18/30\n",
            "32000/32000 [==============================] - 15s 473us/step - loss: 1.0561 - val_loss: 1.5896\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.62618 to 1.58962, saving model to model.ge_en\n",
            "Epoch 19/30\n",
            "32000/32000 [==============================] - 15s 474us/step - loss: 0.9862 - val_loss: 1.5809\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.58962 to 1.58095, saving model to model.ge_en\n",
            "Epoch 20/30\n",
            "32000/32000 [==============================] - 15s 474us/step - loss: 0.9177 - val_loss: 1.5300\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.58095 to 1.52998, saving model to model.ge_en\n",
            "Epoch 21/30\n",
            "32000/32000 [==============================] - 15s 472us/step - loss: 0.8539 - val_loss: 1.5094\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.52998 to 1.50944, saving model to model.ge_en\n",
            "Epoch 22/30\n",
            "32000/32000 [==============================] - 15s 472us/step - loss: 0.7918 - val_loss: 1.4904\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.50944 to 1.49038, saving model to model.ge_en\n",
            "Epoch 23/30\n",
            "32000/32000 [==============================] - 15s 473us/step - loss: 0.7349 - val_loss: 1.4666\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.49038 to 1.46661, saving model to model.ge_en\n",
            "Epoch 24/30\n",
            "32000/32000 [==============================] - 15s 473us/step - loss: 0.6792 - val_loss: 1.4575\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.46661 to 1.45747, saving model to model.ge_en\n",
            "Epoch 25/30\n",
            "32000/32000 [==============================] - 15s 474us/step - loss: 0.6280 - val_loss: 1.4374\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.45747 to 1.43740, saving model to model.ge_en\n",
            "Epoch 26/30\n",
            "32000/32000 [==============================] - 15s 473us/step - loss: 0.5776 - val_loss: 1.4311\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.43740 to 1.43107, saving model to model.ge_en\n",
            "Epoch 27/30\n",
            "32000/32000 [==============================] - 15s 474us/step - loss: 0.5346 - val_loss: 1.4213\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.43107 to 1.42127, saving model to model.ge_en\n",
            "Epoch 28/30\n",
            "32000/32000 [==============================] - 15s 473us/step - loss: 0.4888 - val_loss: 1.4183\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.42127 to 1.41830, saving model to model.ge_en\n",
            "Epoch 29/30\n",
            "32000/32000 [==============================] - 15s 472us/step - loss: 0.4490 - val_loss: 1.4102\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.41830 to 1.41025, saving model to model.ge_en\n",
            "Epoch 30/30\n",
            "32000/32000 [==============================] - 15s 473us/step - loss: 0.4124 - val_loss: 1.4095\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.41025 to 1.40954, saving model to model.ge_en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AKNcATUCARh",
        "colab_type": "text"
      },
      "source": [
        "Let's compare the training loss and the validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2my-LkT62VQo",
        "colab_type": "code",
        "outputId": "7790b37f-144d-40d3-e874-be67672dd908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVmWTIZz2bCE",
        "colab_type": "text"
      },
      "source": [
        "##Make Predictions\n",
        "\n",
        "Let's load the saved model to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxzbqHre3CIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('model.ge_en')\n",
        "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03hQwavR3F7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word(n, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == n:\n",
        "            return word\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dg1upaC3JYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert predictions into text (English)\n",
        "preds_text = []\n",
        "for i in preds:\n",
        "    temp = []\n",
        "    for j in range(len(i)):\n",
        "        t = get_word(i[j], eng_tokenizer)\n",
        "        if j > 0:\n",
        "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "                temp.append('')\n",
        "            else:\n",
        "                temp.append(t)\n",
        "             \n",
        "        else:\n",
        "            if(t == None):\n",
        "                temp.append('')\n",
        "            else:\n",
        "                temp.append(t)            \n",
        "        \n",
        "    preds_text.append(' '.join(temp))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTPaSyUX3OzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diCykH253Wex",
        "colab_type": "code",
        "outputId": "cd019dc3-9626-4b75-e302-f62d41f5b91f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "pred_df.head(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>he wanted to be rich</td>\n",
              "      <td>he wanted to rich</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i love tom</td>\n",
              "      <td>i love tom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>let us go home</td>\n",
              "      <td>lets go home</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i love driving</td>\n",
              "      <td>i love like my</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>this is my dictionary</td>\n",
              "      <td>thats my dictionary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>hi tom good morning</td>\n",
              "      <td>hi tom over</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>why is she so popular</td>\n",
              "      <td>why is so  popular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ill show you my room</td>\n",
              "      <td>ill show you my room</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>did tom oversleep</td>\n",
              "      <td>did tom satisfied</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>keep up the good work</td>\n",
              "      <td>look  good do</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>what happened</td>\n",
              "      <td>what happened</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>toms weak</td>\n",
              "      <td>tom is weak</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>take tom home</td>\n",
              "      <td>take tom home</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>school is out</td>\n",
              "      <td>dinners is over</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>it is pitch dark</td>\n",
              "      <td>its rush hour</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   actual                 predicted\n",
              "0    he wanted to be rich     he wanted to rich    \n",
              "1              i love tom           i love tom     \n",
              "2          let us go home         lets go home     \n",
              "3          i love driving        i love like my    \n",
              "4   this is my dictionary  thats my dictionary     \n",
              "5     hi tom good morning          hi tom over     \n",
              "6   why is she so popular     why is so  popular   \n",
              "7    ill show you my room   ill show you my room   \n",
              "8       did tom oversleep    did tom satisfied     \n",
              "9   keep up the good work         look  good do    \n",
              "10          what happened       what happened      \n",
              "11              toms weak          tom is weak     \n",
              "12          take tom home        take tom home     \n",
              "13          school is out      dinners is over     \n",
              "14       it is pitch dark        its rush hour     "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJYWQ4Ko3dde",
        "colab_type": "code",
        "outputId": "494b8b29-5483-48a8-a300-59c4b7d9868e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "pred_df.tail(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9985</th>\n",
              "      <td>i didnt notice it</td>\n",
              "      <td>i wont care that</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9986</th>\n",
              "      <td>she rode a camel</td>\n",
              "      <td>she took on a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9987</th>\n",
              "      <td>trust me he said</td>\n",
              "      <td>tell as i was</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9988</th>\n",
              "      <td>tom grabbed his bag</td>\n",
              "      <td>tom has his hair</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9989</th>\n",
              "      <td>what was tom like</td>\n",
              "      <td>how did tom like</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9990</th>\n",
              "      <td>it looks like snow</td>\n",
              "      <td>it looks fine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9991</th>\n",
              "      <td>tom works slowly</td>\n",
              "      <td>tom works slowly</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9992</th>\n",
              "      <td>i love rock</td>\n",
              "      <td>i like reading</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9993</th>\n",
              "      <td>our tv isnt working</td>\n",
              "      <td>the  door</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9994</th>\n",
              "      <td>youre a racist</td>\n",
              "      <td>youre a workaholic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>i have one question</td>\n",
              "      <td>answer i missed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>i find swimming fun</td>\n",
              "      <td>is  fun me</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>tom usually wins</td>\n",
              "      <td>tom rarely fishing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>he deserves the prize</td>\n",
              "      <td>he denied the meal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>ill explain later</td>\n",
              "      <td>ill show it later</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     actual                predicted\n",
              "9985      i didnt notice it     i wont care that    \n",
              "9986       she rode a camel        she took on a    \n",
              "9987       trust me he said        tell as i was    \n",
              "9988    tom grabbed his bag     tom has his hair    \n",
              "9989      what was tom like     how did tom like    \n",
              "9990     it looks like snow       it looks fine     \n",
              "9991       tom works slowly    tom works slowly     \n",
              "9992            i love rock      i like reading     \n",
              "9993    our tv isnt working           the  door     \n",
              "9994         youre a racist  youre a workaholic     \n",
              "9995    i have one question     answer i missed     \n",
              "9996    i find swimming fun           is  fun me    \n",
              "9997       tom usually wins  tom rarely fishing     \n",
              "9998  he deserves the prize   he denied the meal    \n",
              "9999      ill explain later    ill show it later    "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HOFQkz93hWd",
        "colab_type": "code",
        "outputId": "3ff50264-7a12-4ea4-f2b0-af0db7d61d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "pred_df.sample(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1816</th>\n",
              "      <td>i come here every year</td>\n",
              "      <td>i come the every house</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5200</th>\n",
              "      <td>we love this school</td>\n",
              "      <td>we love school</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6612</th>\n",
              "      <td>step inside</td>\n",
              "      <td>go inside</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6334</th>\n",
              "      <td>what can i do to help</td>\n",
              "      <td>how can i help you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4439</th>\n",
              "      <td>tom was sarcastic</td>\n",
              "      <td>tom went moving</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1885</th>\n",
              "      <td>tom wasnt funny</td>\n",
              "      <td>tom wasnt funny</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6710</th>\n",
              "      <td>dinner is ready</td>\n",
              "      <td>dinners is ready</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3860</th>\n",
              "      <td>it sure is cold today</td>\n",
              "      <td>its all cold today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8954</th>\n",
              "      <td>whats your hobby</td>\n",
              "      <td>whats your hobby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>im very forgetful</td>\n",
              "      <td>im very broadminded</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8864</th>\n",
              "      <td>come forward</td>\n",
              "      <td>green are</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8991</th>\n",
              "      <td>i need new shoes</td>\n",
              "      <td>i need a shoes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3539</th>\n",
              "      <td>this makes me so mad</td>\n",
              "      <td>this makes me that</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4777</th>\n",
              "      <td>our store isnt open</td>\n",
              "      <td>those are  here</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5874</th>\n",
              "      <td>he turned traitor</td>\n",
              "      <td>he was having to</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      actual                  predicted\n",
              "1816  i come here every year  i come the every house   \n",
              "5200     we love this school        we love school     \n",
              "6612             step inside            go inside      \n",
              "6334   what can i do to help      how can i help you   \n",
              "4439       tom was sarcastic       tom went moving     \n",
              "1885         tom wasnt funny       tom wasnt funny     \n",
              "6710         dinner is ready      dinners is ready     \n",
              "3860   it sure is cold today     its all cold today    \n",
              "8954        whats your hobby      whats your hobby     \n",
              "781        im very forgetful   im very broadminded     \n",
              "8864            come forward            green are      \n",
              "8991        i need new shoes         i need a shoes    \n",
              "3539    this makes me so mad     this makes me that    \n",
              "4777     our store isnt open        those are  here    \n",
              "5874       he turned traitor       he was having to    "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}