{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Elmo Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramapriyakp/Portfolio/blob/master/NLP/Elmo_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkUHjnnzNyuK",
        "colab_type": "text"
      },
      "source": [
        "#Keras sentiment analysis with Elmo Embeddings\n",
        "\n",
        " One of the recent trends in Natural Language Processing is transfer learning. Transfer learning allows NLP models to learn more from fewer examples. In this notebook, we experiment with so-called ELMo Embeddings, a new approach to word embeddings that relies on a large unlabelled text corpus to understand word meaning in context. \n",
        " \n",
        "##Preparation\n",
        "\n",
        "Let's first install and import all the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r51hHH_oOQbm",
        "colab_type": "code",
        "outputId": "1b18e01d-d158-4125-f86d-ab9d0ae33e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RKcUR3eOPdn",
        "colab_type": "code",
        "outputId": "bf6c68ac-e913-4f64-cfad-e63b172ba980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/NLP"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NLP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CZzLoL-O8jF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukayAM2QPBVX",
        "colab_type": "code",
        "outputId": "25943418-cf9c-4754-fc9e-7f662325a84d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import backend\n",
        "\n",
        "sess = tf.Session()\n",
        "backend.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx27mALrPGmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elmo_model = hub.Module(\"https://alpha.tfhub.dev/google/elmo/2\", trainable=True)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(tf.tables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H4BLGBy1TVZ",
        "colab_type": "text"
      },
      "source": [
        "## ELMo Embeddings\n",
        "A quick example will illustrate how ELMo Embeddings work. When we pass to our model a list of sentences (either as strings or as lists of tokens), we get back a list of 1024-dimensional embeddings for every sentence. These are the ELMo embeddings of the tokens in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8pfcdMjPNSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = elmo_model(\n",
        "    [\"the cat is on the mat\", \"dogs are in the fog\"],\n",
        "    signature=\"default\",\n",
        "    as_dict=True)[\"elmo\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMTMwK-71a0V",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment analysis\n",
        "In this experiment, we're going to build a simple neural network for sentiment analysis. As our training and test data, we use the IMDB movie reviews that come pre-packaged with Keras. We shuffle the reviews and pad all texts to a maximum length of 500."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuDMhKAbPSv9",
        "colab_type": "code",
        "outputId": "34f495e9-2966-42af-91ff-f5101a9a2836",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "VOCABULARY_SIZE = 50000\n",
        "INDEX_FROM = 3\n",
        "START_INDEX = 1\n",
        "OOV_INDEX = 2\n",
        "EMBEDDING_DIM = 300\n",
        "SEQ_LENGTH = 500\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
        "                                                      num_words=VOCABULARY_SIZE,\n",
        "                                                      skip_top=0,\n",
        "                                                      maxlen=None,\n",
        "                                                      seed=113,\n",
        "                                                      start_char=START_INDEX,\n",
        "                                                      oov_char=OOV_INDEX,\n",
        "                                                      index_from=INDEX_FROM)\n",
        "\n",
        "train = list(zip(X_train, y_train))\n",
        "random.shuffle(train)\n",
        "X_train, y_train = zip(*train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=SEQ_LENGTH)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=SEQ_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxSxKdcO1ldQ",
        "colab_type": "text"
      },
      "source": [
        "## Simple embeddings\n",
        "For our baseline, we're going to work with standard word embeddings. These map every token to a 300-dimensional embedding, irrespective of the context in which the token occurs. We'll use the English word embeddings from Facebook Research's MUSE project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pv2Go8hQrWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# English fastText Wikipedia embeddings\n",
        "#!wget https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.en.vec\n",
        "# see https://github.com/facebookresearch/MUSE  Multilingual word Embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV5AHkBb1wfw",
        "colab_type": "text"
      },
      "source": [
        "We load these embeddings and put the ones we need in an embedding matrix, where their row indices correspond to the token indices that Keras has assigned to the tokens in the IMDB corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6-ZcFCRTDf-",
        "colab_type": "code",
        "outputId": "3f3a924a-e9e8-4fed-b602-ad0db38937e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_vectors(embedding_file_path):\n",
        "    print(\"Loading vectors from\", embedding_file_path)\n",
        "    embeddings = []\n",
        "    word2id = {}\n",
        "    with open(embedding_file_path, 'r', encoding='utf-8') as f:\n",
        "        next(f)\n",
        "        for i, line in enumerate(f):\n",
        "            word, emb = line.rstrip().split(' ', 1)\n",
        "            emb = np.fromstring(emb, sep=' ')\n",
        "            assert word not in word2id, 'word found twice'\n",
        "            embeddings.append(emb)\n",
        "            word2id[word] = len(word2id)\n",
        "\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    return embeddings, word2id\n",
        "\n",
        "embeddings_en, embedding_word2id_en = load_vectors(\"wiki.multi.en.vec\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading vectors from wiki.multi.en.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2IlqjsRTFyK",
        "colab_type": "code",
        "outputId": "3b89d925-1630-423a-dbfe-6b6ab6b8dc11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def create_embedding_matrix(target_word2id, embedding_word2id, embeddings, num_rows, num_columns):\n",
        "    embedding_matrix = np.zeros((num_rows, num_columns))\n",
        "    for word, i in target_word2id.items():\n",
        "        if i >= num_rows:\n",
        "            continue\n",
        "        if word in embedding_word2id: \n",
        "            embedding_matrix[i] = embeddings[embedding_word2id[word]]\n",
        "    return embedding_matrix\n",
        "\n",
        "word2id_en = imdb.get_word_index()\n",
        "word2id_en = {k:(v+INDEX_FROM) for k,v in word2id_en.items()}\n",
        "word2id_en[\"<PAD>\"] = 0\n",
        "word2id_en[\"<START>\"] = START_INDEX\n",
        "word2id_en[\"<UNK>\"] = OOV_INDEX\n",
        "\n",
        "embedding_matrix_en = create_embedding_matrix(word2id_en, embedding_word2id_en, \n",
        "                                              embeddings_en, VOCABULARY_SIZE+INDEX_FROM-1, EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifIuoWx818Gm",
        "colab_type": "text"
      },
      "source": [
        "## Models\n",
        "We build two models for text classification, which are identical apart from the first layer. Our basic model has a simple embedding layer, were tokens are mapped to their static embeddings. Our ELMo model has a more complex first layer, where the static embedding for each token is concatenated to the ELMo embedding for that token in the relevant context. This results in a 1,324-dimensional embedding for each token in context. In both models, this embedding layer is followed by a simple convolution with kernel size 3, a maximum pooling operation, a dense layer, and finally a final layer that predicts the sentiment of each text as a number between 0 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFtOryxTMUDQ",
        "colab_type": "text"
      },
      "source": [
        "**ELMo (Embeddings from Language Models)** design uses a deep bidirectional LSTM language model for learning words and their context. The deep BiLSTM architecture allows ELMo to learn more context-dependent aspects of word meanings in the higher layers along with syntax aspects in lower layers. This results in better word embeddings, and different representations of a word depending on the context it appears in (especially useful for homographs).\n",
        "\n",
        "**BERT (Bidirectional Encoder Representations from Transformers)** builds on top of the bidirectional idea from ELMo, but uses the relatively new transformer architecture to compute word embeddings. It has been shown to produce excellent word embeddings, achieving state-of-the-art results on various NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd4e0q6LLO7C",
        "colab_type": "text"
      },
      "source": [
        "ELMo representations are:\n",
        "\n",
        "*  __Contextual__: The representation for each word depends on the entire context in which it is used.\n",
        "*  __Deep__: The word representations combine all layers of a deep pre-trained neural network.\n",
        "*  __Character based__: ELMo representations are purely character based, allowing the network to use morphological clues to form robust representations for out-of-vocabulary tokens unseen in training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlwrYkiTTao0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Lambda, Input\n",
        "from keras.layers import Flatten, Concatenate\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "ELMO_EMBEDDING_DIM = 1024\n",
        "\n",
        "def ElmoEmbedding(x):\n",
        "    y = elmo_model(tf.squeeze(x), signature=\"default\", as_dict=True)[\"elmo\"]\n",
        "    return y\n",
        "\n",
        "def create_basic_model():\n",
        "    sequence = Input(shape=(500,))\n",
        "    embedding = Embedding(VOCABULARY_SIZE+INDEX_FROM-1, EMBEDDING_DIM, input_length=SEQ_LENGTH, \n",
        "                              weights=[embedding_matrix_en], trainable=False)(sequence)\n",
        "        \n",
        "    conv = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(embedding)\n",
        "    pool = MaxPooling1D(pool_size=SEQ_LENGTH)(conv)\n",
        "    flat = Flatten()(pool)\n",
        "    dense = Dense(250, activation='relu')(flat)\n",
        "    prediction = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "    model = Model(inputs=sequence, outputs=prediction)\n",
        "    optimizer = Adam(lr=0.0001, decay=1e-3)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    #print(model.summary())\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbCGG0CvVNu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_elmo_model(): \n",
        "    token_sequence = Input(shape=(1,), dtype=\"string\", name=\"elmo_input\")\n",
        "    index_sequence = Input(shape=(SEQ_LENGTH,), name=\"standard_input\")\n",
        "\n",
        "    embedding1 = Lambda(ElmoEmbedding, output_shape=(SEQ_LENGTH, ELMO_EMBEDDING_DIM,))(token_sequence)\n",
        "    embedding2 = Embedding(VOCABULARY_SIZE+INDEX_FROM-1, EMBEDDING_DIM, input_length=SEQ_LENGTH, \n",
        "                          weights=[embedding_matrix_en], trainable=False)(index_sequence)\n",
        "    embedding = Concatenate()([embedding1, embedding2])\n",
        "        \n",
        "    conv = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(embedding)\n",
        "    pool = MaxPooling1D(pool_size=SEQ_LENGTH)(conv)\n",
        "    flat = Flatten()(pool)\n",
        "    dense = Dense(250, activation='relu')(flat)\n",
        "    prediction = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "    model = Model(inputs=[index_sequence, token_sequence], outputs=prediction)\n",
        "    optimizer = Adam(lr=0.00001, decay=1e-3)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    #print(model.summary())\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34T7ckpV2D6n",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "We train both models for a maximum of 100 epochs, but we stop earlier when the validation loss hasn't improved for two epochs. We save and evaluate the model with the lowest validation loss. Although we didn't make a big effort to tune the learning rate, we did find that the ELMo model benefits from having a much smaller initial learning rate than the basic model. We use the same decay rate for both models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1em7bl3gVPwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from keras.models import load_model\n",
        "\n",
        "def train_basic_model(model, X_train, y_train, X_val, y_val, X_test, y_test): \n",
        "    batch_size = 16\n",
        "    earlystop = EarlyStopping(monitor='val_loss', patience=2) \n",
        "    checkpoint = ModelCheckpoint('basic_model.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "    model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
        "              epochs=100, batch_size=batch_size, callbacks=[earlystop, checkpoint])\n",
        "    model.load_weights(\"basic_model.hdf5\")\n",
        "    scores = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    return scores[1]*100\n",
        "\n",
        "\n",
        "def train_elmo_model(model, X_train, E_train, y_train, X_val, E_val, y_val, X_test, E_test, y_test): \n",
        "    batch_size = 16\n",
        "    earlystop = EarlyStopping(monitor='val_loss', patience=2)        \n",
        "    checkpoint = ModelCheckpoint('elmo_model.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "    model.fit([X_train, E_train], y_train, validation_data=([X_val, E_val], y_val), \n",
        "              epochs=100, batch_size=batch_size, callbacks=[earlystop, checkpoint])\n",
        "    model.load_weights('elmo_model.hdf5')\n",
        "    scores = model.evaluate([X_test, E_test], y_test, batch_size=batch_size)\n",
        "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "    return scores[1]*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ManOaARd2Jue",
        "colab_type": "text"
      },
      "source": [
        "Because the ELMo model is quite slow, we chose to work with relatively small datasets. We train on just 200 training examples, validate the model on another 200 examples after each epoch, and test its final performance on 500 test examples. We repeat this process 10 times, and choose the training and validation examples randomly from the larger IMDB training set on each iteration. The ELMo model is trained, validated and tested on exactly the same examples as the basic model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbYJ7B6LVXG4",
        "colab_type": "code",
        "outputId": "bda7b7e6-e06a-442f-abf6-6c42a2dfc969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "elmo_accuracies = []\n",
        "basic_accuracies = []\n",
        "\n",
        "test_size = 500\n",
        "validation_size = 200\n",
        "training_size = 200\n",
        "id2word_en = {v:k for k,v in word2id_en.items()}\n",
        "\n",
        "for i in range(10):     \n",
        "    \n",
        "    train = list(zip(X_train, y_train))\n",
        "    test = list(zip(X_test, y_test))\n",
        "    random.shuffle(train)\n",
        "    X_train, y_train = zip(*train)\n",
        "    X_test, y_test = zip(*test)\n",
        "    \n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "    \n",
        "    X_test = np.array(X_test)\n",
        "    y_test = np.array(y_test)\n",
        "            \n",
        "    train_texts = [\" \".join([id2word_en[idx] for idx in seq]) for seq in X_train[:training_size]]\n",
        "    test_texts = [\" \".join([id2word_en[idx] for idx in seq]) for seq in X_test[:test_size]]\n",
        "    val_texts = [\" \".join([id2word_en[idx] for idx in seq]) for seq in X_test[test_size:test_size+validation_size]]\n",
        " \n",
        "    E_train = np.array(train_texts)\n",
        "    E_test = np.array(test_texts)\n",
        "    E_val = np.array(val_texts)\n",
        "        \n",
        "    model_baseline = create_basic_model()\n",
        "    basic_acc = train_basic_model(model_baseline, X_train[:training_size], y_train[:training_size], \n",
        "                                  X_test[test_size:test_size+validation_size],\n",
        "                                  y_test[test_size:test_size+validation_size], \n",
        "                                  X_test[:test_size], y_test[:test_size])\n",
        "    basic_accuracies.append(basic_acc)\n",
        "\n",
        "    model_elmo = create_elmo_model()\n",
        "    elmo_acc = train_elmo_model(model_elmo, X_train[:training_size], E_train, y_train[:training_size],\n",
        "                                X_test[test_size:test_size+validation_size], \n",
        "                                E_val, y_test[test_size:test_size+validation_size], \n",
        "                                X_test[:test_size], E_test, y_test[:test_size])\n",
        "    elmo_accuracies.append(elmo_acc)\n",
        "        \n",
        "    print(basic_accuracies)\n",
        "    print(elmo_accuracies)\n",
        "    \n",
        "print(np.mean(basic_accuracies))\n",
        "print(np.mean(elmo_accuracies))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.6888 - acc: 0.5600 - val_loss: 0.6950 - val_acc: 0.5150\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6679 - acc: 0.5600 - val_loss: 0.6933 - val_acc: 0.5150\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6426 - acc: 0.5650 - val_loss: 0.6839 - val_acc: 0.5400\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6053 - acc: 0.7400 - val_loss: 0.6743 - val_acc: 0.6150\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.5348 - acc: 0.9450 - val_loss: 0.6717 - val_acc: 0.5500\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.4289 - acc: 0.9650 - val_loss: 0.6894 - val_acc: 0.5300\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.3072 - acc: 0.9900 - val_loss: 0.6261 - val_acc: 0.6500\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.1792 - acc: 1.0000 - val_loss: 0.5878 - val_acc: 0.7000\n",
            "Epoch 9/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.1004 - acc: 1.0000 - val_loss: 0.5764 - val_acc: 0.6850\n",
            "Epoch 10/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0556 - acc: 1.0000 - val_loss: 0.5779 - val_acc: 0.7150\n",
            "Epoch 11/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0340 - acc: 1.0000 - val_loss: 0.6298 - val_acc: 0.6550\n",
            "500/500 [==============================] - 0s 406us/step\n",
            "Accuracy: 73.20%\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 117s 583ms/step - loss: 0.7253 - acc: 0.5000 - val_loss: 0.6653 - val_acc: 0.5800\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 114s 569ms/step - loss: 0.4489 - acc: 0.8350 - val_loss: 0.7231 - val_acc: 0.5500\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 114s 570ms/step - loss: 0.2443 - acc: 0.9250 - val_loss: 0.6034 - val_acc: 0.6700\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 114s 572ms/step - loss: 0.0746 - acc: 1.0000 - val_loss: 0.5566 - val_acc: 0.7300\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 114s 572ms/step - loss: 0.0313 - acc: 1.0000 - val_loss: 0.6144 - val_acc: 0.7000\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 114s 569ms/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.9426 - val_acc: 0.6150\n",
            "500/500 [==============================] - 141s 281ms/step\n",
            "Accuracy: 72.40%\n",
            "[73.2]\n",
            "[72.39999999999999]\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 2s 9ms/step - loss: 0.7079 - acc: 0.4650 - val_loss: 0.6911 - val_acc: 0.5150\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6933 - acc: 0.5050 - val_loss: 0.6942 - val_acc: 0.5150\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6733 - acc: 0.7200 - val_loss: 0.6886 - val_acc: 0.6500\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6581 - acc: 0.6250 - val_loss: 0.6837 - val_acc: 0.6100\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6448 - acc: 0.5650 - val_loss: 0.6801 - val_acc: 0.6400\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.5948 - acc: 0.7900 - val_loss: 0.6711 - val_acc: 0.5800\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.5051 - acc: 0.9950 - val_loss: 0.6529 - val_acc: 0.6400\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.3951 - acc: 0.9950 - val_loss: 0.6514 - val_acc: 0.5400\n",
            "Epoch 9/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.2687 - acc: 0.9950 - val_loss: 0.6283 - val_acc: 0.6100\n",
            "Epoch 10/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.1599 - acc: 1.0000 - val_loss: 0.5858 - val_acc: 0.7200\n",
            "Epoch 11/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0849 - acc: 1.0000 - val_loss: 0.5562 - val_acc: 0.7800\n",
            "Epoch 12/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0456 - acc: 1.0000 - val_loss: 0.5786 - val_acc: 0.6800\n",
            "Epoch 13/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0277 - acc: 1.0000 - val_loss: 0.5349 - val_acc: 0.7700\n",
            "Epoch 14/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.5258 - val_acc: 0.7600\n",
            "Epoch 15/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.5308 - val_acc: 0.7450\n",
            "Epoch 16/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.5195 - val_acc: 0.7600\n",
            "Epoch 17/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.5177 - val_acc: 0.7650\n",
            "Epoch 18/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.5182 - val_acc: 0.7650\n",
            "Epoch 19/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.5103 - val_acc: 0.7650\n",
            "Epoch 20/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.5132 - val_acc: 0.7750\n",
            "Epoch 21/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.5093 - val_acc: 0.7700\n",
            "Epoch 22/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.5105 - val_acc: 0.7750\n",
            "Epoch 23/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.5113 - val_acc: 0.7650\n",
            "500/500 [==============================] - 0s 358us/step\n",
            "Accuracy: 74.40%\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 117s 585ms/step - loss: 0.7294 - acc: 0.5800 - val_loss: 0.6776 - val_acc: 0.5350\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 114s 571ms/step - loss: 0.4669 - acc: 0.7250 - val_loss: 0.6362 - val_acc: 0.5500\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 114s 572ms/step - loss: 0.2460 - acc: 0.9350 - val_loss: 0.5454 - val_acc: 0.7350\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 114s 572ms/step - loss: 0.0878 - acc: 1.0000 - val_loss: 0.4900 - val_acc: 0.7950\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 115s 573ms/step - loss: 0.0312 - acc: 1.0000 - val_loss: 0.4642 - val_acc: 0.7800\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 114s 570ms/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.4529 - val_acc: 0.8000\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 114s 570ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.4518 - val_acc: 0.7850\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 113s 567ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.4527 - val_acc: 0.8000\n",
            "Epoch 9/100\n",
            "200/200 [==============================] - 114s 569ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.4536 - val_acc: 0.7850\n",
            "500/500 [==============================] - 141s 281ms/step\n",
            "Accuracy: 77.80%\n",
            "[73.2, 74.4]\n",
            "[72.39999999999999, 77.8]\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 2s 9ms/step - loss: 0.6922 - acc: 0.5200 - val_loss: 0.6916 - val_acc: 0.5250\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6693 - acc: 0.6000 - val_loss: 0.6946 - val_acc: 0.4850\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6440 - acc: 0.6300 - val_loss: 0.6836 - val_acc: 0.6000\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.5909 - acc: 0.8500 - val_loss: 0.6924 - val_acc: 0.4950\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.5358 - acc: 0.9100 - val_loss: 0.7283 - val_acc: 0.4850\n",
            "500/500 [==============================] - 0s 356us/step\n",
            "Accuracy: 61.80%\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 117s 585ms/step - loss: 0.7724 - acc: 0.5050 - val_loss: 0.6812 - val_acc: 0.5150\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 114s 572ms/step - loss: 0.5119 - acc: 0.7450 - val_loss: 0.6039 - val_acc: 0.6900\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 114s 571ms/step - loss: 0.2792 - acc: 0.9950 - val_loss: 0.5649 - val_acc: 0.6750\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 114s 568ms/step - loss: 0.1011 - acc: 0.9950 - val_loss: 0.7657 - val_acc: 0.5850\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 114s 568ms/step - loss: 0.0320 - acc: 1.0000 - val_loss: 0.5542 - val_acc: 0.7000\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 114s 569ms/step - loss: 0.0137 - acc: 1.0000 - val_loss: 0.5179 - val_acc: 0.7650\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 114s 572ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.5416 - val_acc: 0.7250\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 114s 572ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.5190 - val_acc: 0.7600\n",
            "500/500 [==============================] - 138s 277ms/step\n",
            "Accuracy: 75.60%\n",
            "[73.2, 74.4, 61.8]\n",
            "[72.39999999999999, 77.8, 75.6]\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 3s 14ms/step - loss: 0.6960 - acc: 0.5300 - val_loss: 0.6941 - val_acc: 0.4850\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6791 - acc: 0.6200 - val_loss: 0.6935 - val_acc: 0.4850\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6554 - acc: 0.5900 - val_loss: 0.6899 - val_acc: 0.4900\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6357 - acc: 0.6550 - val_loss: 0.6820 - val_acc: 0.5150\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.5734 - acc: 0.7600 - val_loss: 0.7023 - val_acc: 0.4850\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.4955 - acc: 0.9350 - val_loss: 0.6776 - val_acc: 0.5100\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.3803 - acc: 0.9600 - val_loss: 0.6250 - val_acc: 0.6800\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.2447 - acc: 1.0000 - val_loss: 0.5968 - val_acc: 0.6550\n",
            "Epoch 9/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.1489 - acc: 1.0000 - val_loss: 0.5760 - val_acc: 0.7250\n",
            "Epoch 10/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0786 - acc: 1.0000 - val_loss: 0.5558 - val_acc: 0.7250\n",
            "Epoch 11/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0437 - acc: 1.0000 - val_loss: 0.5464 - val_acc: 0.7150\n",
            "Epoch 12/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0246 - acc: 1.0000 - val_loss: 0.5359 - val_acc: 0.7000\n",
            "Epoch 13/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.5373 - val_acc: 0.7350\n",
            "Epoch 14/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.5330 - val_acc: 0.7350\n",
            "Epoch 15/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.5314 - val_acc: 0.7000\n",
            "Epoch 16/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.5314 - val_acc: 0.7200\n",
            "Epoch 17/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.5378 - val_acc: 0.7250\n",
            "Epoch 18/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.5318 - val_acc: 0.7000\n",
            "500/500 [==============================] - 0s 347us/step\n",
            "Accuracy: 74.20%\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 116s 579ms/step - loss: 0.7873 - acc: 0.5450 - val_loss: 0.6815 - val_acc: 0.5200\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 113s 563ms/step - loss: 0.5541 - acc: 0.7400 - val_loss: 0.6531 - val_acc: 0.5950\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 113s 563ms/step - loss: 0.3841 - acc: 0.8400 - val_loss: 0.8612 - val_acc: 0.5200\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 112s 561ms/step - loss: 0.2471 - acc: 0.9100 - val_loss: 0.5491 - val_acc: 0.7250\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 112s 561ms/step - loss: 0.1060 - acc: 0.9900 - val_loss: 0.5655 - val_acc: 0.6800\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 115s 573ms/step - loss: 0.0395 - acc: 1.0000 - val_loss: 0.6088 - val_acc: 0.6550\n",
            "500/500 [==============================] - 141s 282ms/step\n",
            "Accuracy: 70.80%\n",
            "[73.2, 74.4, 61.8, 74.2]\n",
            "[72.39999999999999, 77.8, 75.6, 70.8]\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 3s 15ms/step - loss: 0.6951 - acc: 0.5250 - val_loss: 0.6968 - val_acc: 0.5150\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6796 - acc: 0.5350 - val_loss: 0.6922 - val_acc: 0.5150\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6576 - acc: 0.6800 - val_loss: 0.6854 - val_acc: 0.6150\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6302 - acc: 0.7600 - val_loss: 0.6831 - val_acc: 0.5150\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.5817 - acc: 0.8050 - val_loss: 0.6708 - val_acc: 0.5950\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.4975 - acc: 0.9850 - val_loss: 0.6663 - val_acc: 0.5300\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.3856 - acc: 1.0000 - val_loss: 0.6475 - val_acc: 0.5650\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.2566 - acc: 1.0000 - val_loss: 0.6334 - val_acc: 0.5750\n",
            "Epoch 9/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.1407 - acc: 1.0000 - val_loss: 0.5967 - val_acc: 0.6850\n",
            "Epoch 10/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0737 - acc: 1.0000 - val_loss: 0.5765 - val_acc: 0.7000\n",
            "Epoch 11/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0396 - acc: 1.0000 - val_loss: 0.5686 - val_acc: 0.6950\n",
            "Epoch 12/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0236 - acc: 1.0000 - val_loss: 0.5603 - val_acc: 0.7150\n",
            "Epoch 13/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0154 - acc: 1.0000 - val_loss: 0.5795 - val_acc: 0.6850\n",
            "Epoch 14/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0110 - acc: 1.0000 - val_loss: 0.5594 - val_acc: 0.6850\n",
            "Epoch 15/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.6174 - val_acc: 0.6500\n",
            "Epoch 16/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.5561 - val_acc: 0.6850\n",
            "Epoch 17/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.5776 - val_acc: 0.6800\n",
            "Epoch 18/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.5793 - val_acc: 0.6800\n",
            "500/500 [==============================] - 0s 346us/step\n",
            "Accuracy: 72.60%\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 118s 591ms/step - loss: 0.7347 - acc: 0.5200 - val_loss: 0.7212 - val_acc: 0.4850\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 114s 572ms/step - loss: 0.5703 - acc: 0.7100 - val_loss: 0.7233 - val_acc: 0.4900\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 115s 573ms/step - loss: 0.4170 - acc: 0.7900 - val_loss: 0.5768 - val_acc: 0.6850\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 115s 573ms/step - loss: 0.1791 - acc: 0.9850 - val_loss: 0.5518 - val_acc: 0.7050\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 114s 569ms/step - loss: 0.0636 - acc: 1.0000 - val_loss: 0.4964 - val_acc: 0.7450\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 114s 569ms/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.4820 - val_acc: 0.7700\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 114s 570ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.4877 - val_acc: 0.7900\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 114s 568ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.4921 - val_acc: 0.7850\n",
            "500/500 [==============================] - 140s 280ms/step\n",
            "Accuracy: 75.60%\n",
            "[73.2, 74.4, 61.8, 74.2, 72.6]\n",
            "[72.39999999999999, 77.8, 75.6, 70.8, 75.6]\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 0.6944 - acc: 0.5250 - val_loss: 0.6954 - val_acc: 0.5150\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6750 - acc: 0.5650 - val_loss: 0.6906 - val_acc: 0.5150\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6518 - acc: 0.5800 - val_loss: 0.6881 - val_acc: 0.5150\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6200 - acc: 0.5650 - val_loss: 0.6738 - val_acc: 0.5150\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.5629 - acc: 0.9650 - val_loss: 0.6529 - val_acc: 0.5800\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.4883 - acc: 0.9350 - val_loss: 0.6331 - val_acc: 0.6150\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.3855 - acc: 0.9250 - val_loss: 0.5934 - val_acc: 0.7600\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.2520 - acc: 0.9950 - val_loss: 0.5919 - val_acc: 0.6850\n",
            "Epoch 9/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.1654 - acc: 1.0000 - val_loss: 0.5417 - val_acc: 0.7550\n",
            "Epoch 10/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0872 - acc: 1.0000 - val_loss: 0.5360 - val_acc: 0.7350\n",
            "Epoch 11/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0519 - acc: 1.0000 - val_loss: 0.5081 - val_acc: 0.7550\n",
            "Epoch 12/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0314 - acc: 1.0000 - val_loss: 0.5045 - val_acc: 0.7650\n",
            "Epoch 13/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0211 - acc: 1.0000 - val_loss: 0.5542 - val_acc: 0.7200\n",
            "Epoch 14/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.5279 - val_acc: 0.7450\n",
            "500/500 [==============================] - 0s 381us/step\n",
            "Accuracy: 74.20%\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 118s 590ms/step - loss: 0.7443 - acc: 0.5250 - val_loss: 0.7119 - val_acc: 0.5050\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 114s 569ms/step - loss: 0.4538 - acc: 0.7650 - val_loss: 0.6192 - val_acc: 0.6800\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 113s 567ms/step - loss: 0.2499 - acc: 0.9350 - val_loss: 0.5797 - val_acc: 0.6500\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 113s 567ms/step - loss: 0.0952 - acc: 1.0000 - val_loss: 0.6723 - val_acc: 0.6550\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 114s 569ms/step - loss: 0.0328 - acc: 1.0000 - val_loss: 0.5275 - val_acc: 0.6950\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 113s 567ms/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.5994 - val_acc: 0.6750\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 114s 568ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.5614 - val_acc: 0.6900\n",
            "500/500 [==============================] - 140s 279ms/step\n",
            "Accuracy: 73.20%\n",
            "[73.2, 74.4, 61.8, 74.2, 72.6, 74.2]\n",
            "[72.39999999999999, 77.8, 75.6, 70.8, 75.6, 73.2]\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 4s 20ms/step - loss: 0.6916 - acc: 0.5500 - val_loss: 0.6996 - val_acc: 0.5150\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6757 - acc: 0.5600 - val_loss: 0.6949 - val_acc: 0.5150\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6570 - acc: 0.5600 - val_loss: 0.6893 - val_acc: 0.5150\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6329 - acc: 0.6600 - val_loss: 0.6826 - val_acc: 0.5150\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.5856 - acc: 0.7850 - val_loss: 0.6664 - val_acc: 0.5400\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.5185 - acc: 0.7700 - val_loss: 0.6470 - val_acc: 0.6400\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.4108 - acc: 1.0000 - val_loss: 0.6370 - val_acc: 0.5500\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.2923 - acc: 1.0000 - val_loss: 0.5929 - val_acc: 0.6500\n",
            "Epoch 9/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.1753 - acc: 1.0000 - val_loss: 0.5443 - val_acc: 0.8050\n",
            "Epoch 10/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.1030 - acc: 1.0000 - val_loss: 0.5220 - val_acc: 0.7850\n",
            "Epoch 11/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0597 - acc: 1.0000 - val_loss: 0.4998 - val_acc: 0.7900\n",
            "Epoch 12/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0338 - acc: 1.0000 - val_loss: 0.5313 - val_acc: 0.7150\n",
            "Epoch 13/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0202 - acc: 1.0000 - val_loss: 0.5166 - val_acc: 0.7300\n",
            "500/500 [==============================] - 0s 372us/step\n",
            "Accuracy: 77.60%\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 119s 594ms/step - loss: 0.7163 - acc: 0.5650 - val_loss: 0.7031 - val_acc: 0.5250\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 113s 563ms/step - loss: 0.4836 - acc: 0.8350 - val_loss: 0.6242 - val_acc: 0.5950\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 112s 559ms/step - loss: 0.2497 - acc: 0.9700 - val_loss: 0.5391 - val_acc: 0.7350\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 112s 561ms/step - loss: 0.0873 - acc: 1.0000 - val_loss: 0.4924 - val_acc: 0.7750\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 113s 563ms/step - loss: 0.0256 - acc: 1.0000 - val_loss: 0.4766 - val_acc: 0.7750\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 112s 561ms/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.4752 - val_acc: 0.7800\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 113s 563ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.4877 - val_acc: 0.7750\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 113s 563ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.4887 - val_acc: 0.7750\n",
            "500/500 [==============================] - 138s 277ms/step\n",
            "Accuracy: 78.60%\n",
            "[73.2, 74.4, 61.8, 74.2, 72.6, 74.2, 77.60000000000001]\n",
            "[72.39999999999999, 77.8, 75.6, 70.8, 75.6, 73.2, 78.60000000000001]\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 4s 21ms/step - loss: 0.6950 - acc: 0.5350 - val_loss: 0.6927 - val_acc: 0.5150\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6750 - acc: 0.5450 - val_loss: 0.6898 - val_acc: 0.5150\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6518 - acc: 0.6600 - val_loss: 0.6855 - val_acc: 0.5150\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6173 - acc: 0.6900 - val_loss: 0.6770 - val_acc: 0.5200\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.5537 - acc: 0.9300 - val_loss: 0.6901 - val_acc: 0.5150\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.4643 - acc: 0.9500 - val_loss: 0.6510 - val_acc: 0.5500\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.3344 - acc: 0.9950 - val_loss: 0.6937 - val_acc: 0.5200\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.2338 - acc: 0.9950 - val_loss: 0.6697 - val_acc: 0.5250\n",
            "500/500 [==============================] - 0s 364us/step\n",
            "Accuracy: 57.80%\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 118s 589ms/step - loss: 0.7010 - acc: 0.5600 - val_loss: 0.6456 - val_acc: 0.6650\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 112s 562ms/step - loss: 0.4091 - acc: 0.9000 - val_loss: 0.5758 - val_acc: 0.7200\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 113s 563ms/step - loss: 0.1695 - acc: 0.9900 - val_loss: 0.6555 - val_acc: 0.5700\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 112s 561ms/step - loss: 0.0805 - acc: 1.0000 - val_loss: 0.5207 - val_acc: 0.7400\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 113s 563ms/step - loss: 0.0245 - acc: 1.0000 - val_loss: 0.4747 - val_acc: 0.7900\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 113s 563ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.4672 - val_acc: 0.7950\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 113s 564ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.5140 - val_acc: 0.7550\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 113s 563ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.4831 - val_acc: 0.8000\n",
            "500/500 [==============================] - 138s 277ms/step\n",
            "Accuracy: 77.60%\n",
            "[73.2, 74.4, 61.8, 74.2, 72.6, 74.2, 77.60000000000001, 57.8]\n",
            "[72.39999999999999, 77.8, 75.6, 70.8, 75.6, 73.2, 78.60000000000001, 77.60000000000001]\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 5s 24ms/step - loss: 0.6991 - acc: 0.4950 - val_loss: 0.6949 - val_acc: 0.4850\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6794 - acc: 0.6050 - val_loss: 0.6898 - val_acc: 0.5600\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6533 - acc: 0.6900 - val_loss: 0.6864 - val_acc: 0.5150\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6231 - acc: 0.8050 - val_loss: 0.6809 - val_acc: 0.5950\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.5738 - acc: 0.7450 - val_loss: 0.6953 - val_acc: 0.4850\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.5139 - acc: 0.7550 - val_loss: 0.6854 - val_acc: 0.5150\n",
            "500/500 [==============================] - 0s 424us/step\n",
            "Accuracy: 62.00%\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 118s 591ms/step - loss: 0.7614 - acc: 0.4950 - val_loss: 0.6507 - val_acc: 0.6050\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 113s 565ms/step - loss: 0.5141 - acc: 0.7450 - val_loss: 0.5969 - val_acc: 0.7050\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 112s 561ms/step - loss: 0.2730 - acc: 0.9650 - val_loss: 0.5349 - val_acc: 0.7700\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 112s 562ms/step - loss: 0.0970 - acc: 1.0000 - val_loss: 0.4882 - val_acc: 0.7750\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 112s 560ms/step - loss: 0.0272 - acc: 1.0000 - val_loss: 0.4777 - val_acc: 0.7800\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 112s 561ms/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.5000 - val_acc: 0.7500\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 112s 560ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.4803 - val_acc: 0.7800\n",
            "500/500 [==============================] - 138s 275ms/step\n",
            "Accuracy: 76.60%\n",
            "[73.2, 74.4, 61.8, 74.2, 72.6, 74.2, 77.60000000000001, 57.8, 62.0]\n",
            "[72.39999999999999, 77.8, 75.6, 70.8, 75.6, 73.2, 78.60000000000001, 77.60000000000001, 76.6]\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 5s 26ms/step - loss: 0.6940 - acc: 0.5350 - val_loss: 0.6960 - val_acc: 0.5150\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6752 - acc: 0.5400 - val_loss: 0.6896 - val_acc: 0.6350\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6525 - acc: 0.8550 - val_loss: 0.6853 - val_acc: 0.5300\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.6186 - acc: 0.7700 - val_loss: 0.6812 - val_acc: 0.6050\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.5481 - acc: 0.9600 - val_loss: 0.6776 - val_acc: 0.5250\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.4435 - acc: 0.9950 - val_loss: 0.6661 - val_acc: 0.5450\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.3249 - acc: 1.0000 - val_loss: 0.6434 - val_acc: 0.6300\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.1989 - acc: 1.0000 - val_loss: 0.6183 - val_acc: 0.6750\n",
            "Epoch 9/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.1101 - acc: 1.0000 - val_loss: 0.5996 - val_acc: 0.7500\n",
            "Epoch 10/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0600 - acc: 1.0000 - val_loss: 0.5938 - val_acc: 0.6800\n",
            "Epoch 11/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0344 - acc: 1.0000 - val_loss: 0.5839 - val_acc: 0.7500\n",
            "Epoch 12/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0217 - acc: 1.0000 - val_loss: 0.5849 - val_acc: 0.7400\n",
            "Epoch 13/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.5760 - val_acc: 0.7300\n",
            "Epoch 14/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0106 - acc: 1.0000 - val_loss: 0.5799 - val_acc: 0.7400\n",
            "Epoch 15/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0083 - acc: 1.0000 - val_loss: 0.5735 - val_acc: 0.7250\n",
            "Epoch 16/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.5737 - val_acc: 0.7400\n",
            "Epoch 17/100\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.5736 - val_acc: 0.7350\n",
            "500/500 [==============================] - 0s 390us/step\n",
            "Accuracy: 73.20%\n",
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/100\n",
            "200/200 [==============================] - 118s 591ms/step - loss: 0.7362 - acc: 0.5300 - val_loss: 0.6282 - val_acc: 0.7600\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - 112s 560ms/step - loss: 0.4706 - acc: 0.8250 - val_loss: 0.5717 - val_acc: 0.7200\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - 112s 560ms/step - loss: 0.2491 - acc: 0.9500 - val_loss: 0.5208 - val_acc: 0.7600\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - 112s 560ms/step - loss: 0.1166 - acc: 0.9900 - val_loss: 0.6014 - val_acc: 0.6350\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - 112s 560ms/step - loss: 0.0327 - acc: 1.0000 - val_loss: 0.5091 - val_acc: 0.7800\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 112s 561ms/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.5216 - val_acc: 0.7750\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 112s 561ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.4785 - val_acc: 0.8150\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 112s 562ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.4971 - val_acc: 0.7950\n",
            "Epoch 9/100\n",
            "200/200 [==============================] - 112s 561ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.4830 - val_acc: 0.8200\n",
            "500/500 [==============================] - 138s 275ms/step\n",
            "Accuracy: 75.00%\n",
            "[73.2, 74.4, 61.8, 74.2, 72.6, 74.2, 77.60000000000001, 57.8, 62.0, 73.2]\n",
            "[72.39999999999999, 77.8, 75.6, 70.8, 75.6, 73.2, 78.60000000000001, 77.60000000000001, 76.6, 75.0]\n",
            "70.10000000000001\n",
            "75.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7CSaLcu2YLU",
        "colab_type": "text"
      },
      "source": [
        "## Results\n",
        "When we train a basic sentiment analysis model on just 200 training examples, the results are hit and miss: the accuracies on unseen texts range from just 48% to 78%. When we replace the simple embedding layer by an ELMo embedding layer, however, the model performs about 10% better on average. Its accuracies were also much more consistent, between 73% and 78%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIqe_TEnVvLd",
        "colab_type": "code",
        "outputId": "ff4935a0-4ba8-46f4-c2c3-45c424ae30e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracies = pd.DataFrame({'basic' : basic_accuracies, 'elmo': elmo_accuracies})\n",
        "plt.rcParams['figure.figsize'] = (10,6)\n",
        "accuracies.boxplot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fbf1d0a4d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAFpCAYAAAC4SK2+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4PUk+zkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIIjbzFhVxCX",
        "colab_type": "code",
        "outputId": "290600cd-d7b9-4f5e-94c5-6bc557717d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "source": [
        "# Pair plot\n",
        "plt.scatter(np.zeros(len(basic_accuracies)), basic_accuracies)\n",
        "plt.scatter(np.ones(len(elmo_accuracies)), elmo_accuracies)\n",
        "\n",
        "for i in range(len(basic_accuracies)):\n",
        "    plt.plot( [0,1], [basic_accuracies[i], elmo_accuracies[i]], c='k')\n",
        "\n",
        "plt.xticks([0,1], ['basic', 'elmo'])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAFpCAYAAAC4SK2+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}